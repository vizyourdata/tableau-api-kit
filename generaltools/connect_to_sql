
import pandas as pd
from dotenv import load_dotenv
import os
from sqlalchemy import create_engine


load_dotenv()
def redshift_query(qry):
    import psycopg2
    import pandas as pd
    user = os.environ.get('post_user')
    password = os.environ.get('post_password')
    host = os.environ.get('post_host')
    port = os.environ.get('post_port')
    database = os.environ.get('post_database')
    REDSHIFT_USER = user
    REDSHIFT_PASSWORD = password
    REDSHIFT_HOST = host
    REDSHIFT_PORT = port
    REDSHIFT_DATABASE = database

    from sqlalchemy.engine.url import URL
    
    # Create the connection string
    db_url = URL.create(
        drivername="postgresql+psycopg2",
        username=REDSHIFT_USER,
        password=REDSHIFT_PASSWORD,
        host=REDSHIFT_HOST,
        port=REDSHIFT_PORT,
        database=REDSHIFT_DATABASE
    )

    # Create an engine
    engine = create_engine(
        db_url,
        pool_size=10,      # Set the pool size (e.g., 10)
        max_overflow=20,   # Allow extra connections beyond the pool size (e.g., 20)
        pool_timeout=30,   # Timeout after 30 seconds if no connections are available
        pool_recycle=300  # Recycle connections every 30 minutes (1800 seconds)
        )
    # Use the engine to execute the query and return a DataFrame
        # Using `with` to ensure proper resource cleanup
    with engine.connect() as connection:
        df = pd.read_sql(qry, connection)
    # df = pd.read_sql(qry, engine)
    
    # Optional: You could also explicitly dispose of the engine to clean up connections
    engine.dispose()
    
    return df
